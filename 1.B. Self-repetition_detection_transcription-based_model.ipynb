{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eebbfb1a",
   "metadata": {},
   "source": [
    "# Transcription-based model for detection of self-repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e87541",
   "metadata": {},
   "source": [
    "The following function, `transcription_model_echolalia`, takes as input a transcription of a single- or multi-speaker audio in **.TextGrid** format. The speech of the child under scope should be annotated in a tier named **'AC'**. The function will return a **dataframe** containing the prediction ('repetitive' or 'non-repetitive') for each utterance pair with additional information about the timestamps of the utterances, the transcriptions of both utterances, etcetera. Moreover, a **new .TextGrid** is produced on the basis of the input .TextGrid, that contains annotations of the self-repetitive utterance pairs in two new tiers: 'source' and 'self-repetition'. The path to this new file needs to be specified in the parameter 'output path'. In the parameter 'language', the spoken language needs to be indicated (supported: 'nl' (Dutch) and 'fr' (French))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c7619d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcription_model_selfrep(textgrid_file, output_path, language):\n",
    "    \"\"\"Takes a TextGrid file of the transcription of an audio file as input, \n",
    "    makes a prediction on the label (self-repetitive or not) for each suitable utterance-pair\n",
    "    and returns a dataframe containing these predictions for each pair,\n",
    "    along with an annotation of the predictions in a new TextGrid\"\"\"\n",
    "    \n",
    "    # Import libraries\n",
    "    import pandas as pd\n",
    "    from praatio import textgrid\n",
    "    import re\n",
    "    import spacy\n",
    "    import tgt\n",
    "    \n",
    "    # Load the spacy model and prepare extraction of speech intervals\n",
    "    nlp = spacy.load(f'{language}_core_news_sm')\n",
    "    \n",
    "    def get_entries(tier):\n",
    "        \"\"\"Takes tier name as input and returns the entry tuple for that tier\"\"\"\n",
    "        tg = textgrid.openTextgrid(textgrid_file, False)\n",
    "        return tg.getTier(tier).entries\n",
    "    \n",
    "    def get_speech_intervals(tier):\n",
    "        \"\"\"Takes tier name as input and returns a nested list of the start and end timestamps\n",
    "        of the utterances spoken by the speaker of the tier, filtering out the unintelligible ones\"\"\"\n",
    "        entries = get_entries(tier)\n",
    "        intervals = {}\n",
    "        regex = r\"^(xxx|yyy)\\s?(\\[.+\\])?\\.?$\" # Regex for unintelligible utterances; change if necessary\n",
    "        for entry in entries:\n",
    "            if not re.match(regex, entry.label):\n",
    "                intervals[(entry.start, entry.end)] = entry.label\n",
    "\n",
    "        return intervals  \n",
    "    \n",
    "    # Define functions that prepare the utterances, extract their dependency structure and predict the label\n",
    "    \n",
    "    def preprocess_string(string):\n",
    "        \"\"\"Takes as input a string and outputs a new string where punctuation,\n",
    "        truncated words and fillers have been removed\"\"\"\n",
    "        string = string.translate(str.maketrans('', '', '+?!,/.()[]'))\n",
    "        if language == 'fr':\n",
    "            to_delete = ['euhm', 'euh', 'uhm', 'mmh', 'xxx', 'eh', 'ben', 'hein', 'ah', 'bah', 'oh', 'bon']\n",
    "        elif language == 'nl':\n",
    "            to_delete = ['euhm', 'euh', 'uhm', 'mmh', 'xxx', 'he', 'hè', 'hé', 'ah', 'oh']\n",
    "        string = ' '.join([word for word in string.split() if word not in to_delete and word[-1] != '-' and word[0] != '-'])\n",
    "        return string\n",
    "\n",
    "    def split_sentence(doc):\n",
    "        \"\"\"Takes as input the SpaCy object of the input utterance and segments it in clauses, \n",
    "        while removing conjunctions\"\"\"\n",
    "        sents = \"\"\n",
    "        split_words = []\n",
    "        to_delete = []\n",
    "        children = []\n",
    "\n",
    "        for token in doc:\n",
    "            if token.pos_ in ['CCONJ', 'SCONJ'] and token.i not in children:\n",
    "                children = []\n",
    "                if token.i == 0:  # If conjunction is first token: just delete it\n",
    "                    to_delete.append(token.text)\n",
    "                else:\n",
    "                    split_words.append(token.text)\n",
    "\n",
    "                for child in token.children:\n",
    "                    if child.pos_ in ['CCONJ', 'SCONJ']:\n",
    "                        children.append(child.i)\n",
    "                        to_delete.append(child.text)\n",
    "\n",
    "        doc_list = [word.text for word in doc if word.text not in to_delete]\n",
    "        doc_str = ' '.join(doc_list)\n",
    "\n",
    "        if split_words:\n",
    "            sents = doc_str.split(str(split_words[0]))\n",
    "            new_sents = []\n",
    "            for sent in sents:\n",
    "                sent = sent.strip()\n",
    "                new_sents.append(sent)\n",
    "            sents = new_sents\n",
    "\n",
    "            if len(split_words) > 1:\n",
    "                for token in doc:\n",
    "                    if token.text in split_words[1:len(split_words)]:\n",
    "                        new_sents = []\n",
    "                        for sent in sents:\n",
    "                            new_sents.extend(sent.split(token.text))\n",
    "                            sents = new_sents\n",
    "                sents = [sent.strip() for sent in sents]\n",
    "        else:\n",
    "            sents = doc_str.strip()\n",
    "\n",
    "        return sents\n",
    "\n",
    "    def get_dep_structure(sent_doc):\n",
    "        \"\"\"Takes as input the SpaCy object of a clause \n",
    "        and returns a list of the tokens pertaining to the dependency structure of that clause\"\"\"\n",
    "        words = []\n",
    "        pos_dep = [(token.dep_, token.pos_) for token in sent_doc]\n",
    "\n",
    "        if ('ROOT', 'VERB') in pos_dep or ('cop', 'VERB') in pos_dep or ('cop', 'AUX') in pos_dep:\n",
    "            for token in sent_doc:\n",
    "                if token.dep_ == 'ROOT' and token.pos_ == 'VERB':\n",
    "                    # Append verbal group\n",
    "                    words.append(token.text)\n",
    "                    for child in token.children:\n",
    "                        if child.pos_ == 'AUX':\n",
    "                            words.append(child.text)\n",
    "                if token.dep_ == 'ROOT' and token.pos_ == 'NOUN':\n",
    "                    words.append(token.text)\n",
    "                    for child in token.children:\n",
    "                        if not child.children:\n",
    "                            words.append(child.text)\n",
    "                        # Take only children that are not prepositional groups, i.e., that don't have a\n",
    "                        # preposition as their child. An exception is made for the Dutch and French equivalents of 'of'.\n",
    "                        elif all([grandchild.pos_ != 'ADP' or grandchild.text in ['de', 'van'] for grandchild in child.children]):\n",
    "                            words.append(child.text)\n",
    "                            for grandchild in child.children:\n",
    "                                words.append(grandchild.text)\n",
    "                if token.dep_ in ['obj', 'nsubj', 'obl:arg', 'iobj', 'csubj', 'ccomp', 'xcomp', 'nsubj:pass', 'cop']:\n",
    "                    # Append subject and objects\n",
    "                    words.append(token.text)\n",
    "                    for child in token.children:\n",
    "                        words.append(child.text)\n",
    "                        for grandchild in child.children:\n",
    "                            words.append(grandchild.text)\n",
    "        return words\n",
    "\n",
    "    def get_dep_structures(doc):\n",
    "        \"\"\"Takes as input the SpaCy object of the utterance \n",
    "        and returns a nested list of the words of the dependency structure of each clause\"\"\"\n",
    "        sents = split_sentence(doc)\n",
    "\n",
    "        dep_structures = []\n",
    "        if isinstance(sents, list):  # = more than one clause per utterance; otherwise type(sents) == str\n",
    "            for sent in sents:\n",
    "                sent_doc = nlp(sent)\n",
    "                words = get_dep_structure(sent_doc)\n",
    "                if len(words) > 1:\n",
    "                    dep_structures.append(words)\n",
    "        else:\n",
    "            dep_structures.append(get_dep_structure(nlp(sents)))\n",
    "\n",
    "        return dep_structures\n",
    "\n",
    "    def detect_selfrep(string1, string2):\n",
    "        \"\"\"Takes as input two strings and returns True if they constitute a self-repetition\"\"\"\n",
    "        new_string1 = preprocess_string(string1)\n",
    "        new_string2 = preprocess_string(string2)\n",
    "\n",
    "        doc1 = nlp(new_string1)\n",
    "        doc2 = nlp(new_string2)\n",
    "\n",
    "        if doc1 and doc2 and [token.text for token in doc1] == [token.text for token in doc2]:\n",
    "            return True\n",
    "\n",
    "        for struct in get_dep_structures(doc1):\n",
    "            for struct2 in get_dep_structures(doc2):\n",
    "                if struct and struct == struct2:\n",
    "                    return True\n",
    "\n",
    "        if split_sentence(doc1) == split_sentence(doc2) and isinstance(split_sentence(doc1), str) and 'VERB' in [token.pos_ for token in doc1]:\n",
    "            return True\n",
    "\n",
    "    # Now create the output dataframe and textgrid\n",
    "\n",
    "    child_intervals = get_speech_intervals('AC')\n",
    "    \n",
    "    df = pd.DataFrame(columns=['source_interval', 'repetition_interval',\n",
    "                               'source_transcription', 'rep_transcription', 'predicted_label'])\n",
    "    \n",
    "    source_rep_dict= {}\n",
    "    tg = tgt.io.read_textgrid(textgrid_file, encoding='utf-16')\n",
    "    source_tier = tgt.IntervalTier(start_time=0, name='source')\n",
    "    rep_tier = tgt.IntervalTier(start_time=0, name='self-repetition')\n",
    "\n",
    "    row_df = 0\n",
    "\n",
    "    # Iterate through child_intervals and compare them to other intervals that the same speaker pronounced before:\n",
    "    \n",
    "    for start_source, end_source in child_intervals:\n",
    "\n",
    "        source_int = (start_source, end_source)\n",
    "        source_trans = child_intervals[(start_source, end_source)]\n",
    "        \n",
    "        for start_rep, end_rep in child_intervals:\n",
    "\n",
    "            rep_int = (start_rep, end_rep)\n",
    "            rep_trans = child_intervals[(start_rep, end_rep)]\n",
    "\n",
    "            if start_rep > start_source:  # Compare repetition candidate only with previous utterances\n",
    "\n",
    "\n",
    "                # Get predictions\n",
    "                pred_label = 'non-repetitive'\n",
    "                if detect_selfrep(rep_trans, source_trans):\n",
    "                    pred_label = 'repetitive'\n",
    "\n",
    "                add_rep= False\n",
    "                \n",
    "                # If the prediction is 'repetitive', write the source and repetitive interval to the output TextGrid                \n",
    "                if pred_label == \"repetitive\":\n",
    "                    \n",
    "                    if source_int not in source_rep_dict and not any(source_int in values for values in source_rep_dict.values()):\n",
    "                        source_rep_dict[source_int]= [rep_int]\n",
    "                        source_key= list(source_rep_dict.keys()).index(source_int)\n",
    "                        \n",
    "                        source_interval_tg = tgt.Interval(start_time=float(start_source),\n",
    "                                                          end_time=float(end_source),\n",
    "                                                          text=f'source {source_key + 1}')\n",
    "                        source_tier.add_interval(source_interval_tg)\n",
    "                        add_rep=True\n",
    "\n",
    "                    if not any(rep_int in values for values in source_rep_dict.values()):\n",
    "                        add_rep=True\n",
    "                        if source_int in source_rep_dict:\n",
    "                            source_rep_dict[source_int] += [rep_int]\n",
    "                            source_key= list(source_rep_dict.keys()).index(source_int)\n",
    "                            \n",
    "                        elif any(source_int in values for values in source_rep_dict.values()):\n",
    "                            original_source= [key for key,value in source_rep_dict.items() if source_int in value][0]\n",
    "                            source_rep_dict[original_source]+= [rep_int]\n",
    "                            source_key= list(source_rep_dict.keys()).index(original_source)\n",
    "                    \n",
    "                    if add_rep== True:\n",
    "                        rep_interval_tg = tgt.Interval(start_time=float(start_rep),\n",
    "                                                       end_time=float(end_rep),\n",
    "                                                       text=f'self-repetition {source_key + 1}')\n",
    "                        rep_tier.add_interval(rep_interval_tg)\n",
    "\n",
    "                # Append all features of the utterance pair to the output dataframe:\n",
    "                df.loc[row_df] = [str(source_int), str(rep_int),\n",
    "                                  source_trans, rep_trans, pred_label]\n",
    "                row_df += 1\n",
    "\n",
    "    # Output the textgrid outside of the loop\n",
    "    tg.add_tier(source_tier)\n",
    "    tg.add_tier(rep_tier)\n",
    "    tgt.write_to_file(tg, output_path, format='short')\n",
    "\n",
    "    return df.loc[df['predicted_label'] == 'repetitive']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03906963",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
