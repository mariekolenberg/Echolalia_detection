{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c008b86f",
   "metadata": {},
   "source": [
    "# Audio-based model for detection of self-repetition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b416e9",
   "metadata": {},
   "source": [
    "The following function, `audio_model_self_repetition` takes as input a **single or multi-speaker audio file** and a **.TextGrid diarization** of that audio. The .TextGrid should contain at least one tier, named **'AC'** (Autistic Child), where the speech of the child of whom echolalic utterances will be detected is indicated in **non-empty intervals** (e.g. indicated by 'speech', 'AC'...). Each utterance of the AC will be compared with previous utterances of the AC and a label ('repetitive' or 'non-repetitive') will be predicted. The output is a **dataframe** containing the predictions for each pair of utterances of the AC containing the timestamps of both utterances. Furthermore, a new **.TextGrid** is created on the basis of the input .TextGrid, containing the annotations of the predicted source and echolalic intervals on two different tiers ('source' and 'repetition'). The output path to this file must be specified in the parameter 'output_path'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50834ff",
   "metadata": {},
   "source": [
    "*Note*: the current trained classifier does not yet achieve satisfying results. This script will be adapted with a new version of the classifier once developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c48b6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_model_self_repetition(input_audio= None, textgrid_file= None, output_path= None):\n",
    "    \"\"\"Takes an audio file and its diarization in .TextGrid as input, \n",
    "    makes a prediction on the label (self-repetitive or not) for each suitable utterance-pair\n",
    "    and returns a dataframe containing these predictions for each pair,\n",
    "    along with an annotation of the predictions in a new TextGrid\"\"\"\n",
    "    \n",
    "    \n",
    "    # Load libraries\n",
    "    import pickle\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    import pandas as pd\n",
    "    from pydub import AudioSegment\n",
    "    from praatio import textgrid\n",
    "    import librosa\n",
    "    import numpy as np\n",
    "    from dtw import dtw\n",
    "    import sklearn.preprocessing\n",
    "    import itertools\n",
    "    import tgt\n",
    "        \n",
    "        \n",
    "    # EXTRACT SPEECH INTERVALS\n",
    "    \n",
    "    input_tg= textgrid.openTextgrid(textgrid_file, includeEmptyIntervals= True)\n",
    "    \n",
    "    def get_speech_intervals(tier):\n",
    "        \"\"\"Takes tier name as input\n",
    "        and returns a nested list of the start and end timestamps\n",
    "        of the utterances spoken by the speaker of the tier,\n",
    "        filtering out the unintelligible ones\"\"\"\n",
    "        entries= input_tg.getTier(tier).entries\n",
    "        intervals= [[entry.start, entry.end] for entry in entries]\n",
    "                \n",
    "        return intervals \n",
    "    \n",
    "    child_intervals= get_speech_intervals('AC')\n",
    "    \n",
    "    # PREPARE THE AUDIO PROCESSING\n",
    "    \n",
    "    def audio_segment_to_np(audiosegment):\n",
    "        \"\"\"Converts an 'Audiosegment' object of the 'audiosegment' library\n",
    "        into a numpy array\"\"\"\n",
    "        samples = audiosegment.get_array_of_samples() # function of AudioSegment library\n",
    "        samples = np.array(samples)\n",
    "        samples = samples.astype(np.float32) / np.iinfo(samples.dtype).max\n",
    "        return samples\n",
    "    \n",
    "    # DEFINE FUNCTIONS FOR FEATURE EXTRACTION FROM AUDIO SIGNAL\n",
    "    \n",
    "    n_fft = 2048\n",
    "    hop_length= 512\n",
    "    \n",
    "    features= ['mfcc', 'melspectrogram', 'lpc']\n",
    "    \n",
    "    def compute_lpc_2d(y, sr, order=13, frame_length=n_fft, hop_length=hop_length):\n",
    "        # so that lpc coefficients are also computed per frame just as for mfcc and melspectrogram\n",
    "        \n",
    "        # Frame the audio signal\n",
    "        y_padded = np.pad(y, (0, frame_length - len(y) % hop_length), mode='constant')\n",
    "        frames = librosa.util.frame(y_padded, frame_length=frame_length, hop_length=hop_length).T\n",
    "        # Compute LPC for each frame\n",
    "        lpc_coeffs = [librosa.lpc(frame, order=order) for frame in frames]\n",
    "        # Stack LPC coefficients into a 2D array\n",
    "        lpc_2d = np.vstack(lpc_coeffs)\n",
    "        return lpc_2d\n",
    "    \n",
    "    def get_feature_array(feature, segment, sr):\n",
    "        if feature== 'mfcc':\n",
    "            # use 'audio_segment_to_np()' function previously defined\n",
    "            feature_vector= librosa.feature.mfcc(y= audio_segment_to_np(segment), n_mfcc=13, hop_length= hop_length, n_fft= n_fft, sr=sr).squeeze()\n",
    "        elif feature== 'melspectrogram':\n",
    "            feature_vector= librosa.feature.melspectrogram(y= audio_segment_to_np(segment), hop_length= hop_length, n_fft= n_fft, sr=sr, n_mels=32).squeeze()\n",
    "        elif feature== 'lpc':\n",
    "            feature_vector= compute_lpc_2d(y= audio_segment_to_np(segment), sr=sr).squeeze().T\n",
    "        \n",
    "        ready_feature_vector= sklearn.preprocessing.minmax_scale(feature_vector, axis=1)\n",
    "                \n",
    "        return ready_feature_vector\n",
    "\n",
    "    \n",
    "    # LOAD THE AUDIO FILE \n",
    "    \n",
    "    audio = AudioSegment.from_wav(input_audio) # Load the audio and create an 'Audiosegment' object\n",
    "    sr = librosa.load(input_audio)[1] # Get sample rate\n",
    "    \n",
    "    \n",
    "    # CREATE THE FEATURE DATAFRAME:\n",
    "    \n",
    "    # Initalize dataframe\n",
    "    df= pd.DataFrame(columns=['source_int', 'rep_int'] +[f'DTW_{feature}' for feature in features] \\\n",
    "                     + [f'DTW_{f1}_{f2}' for f1,f2 in list(itertools.combinations(features,2))]+ ['DTW_combined'])\n",
    "    row_df= 0\n",
    "    \n",
    "    \n",
    "    # Iterate through child_intervals and compare them to other intervals that the same speaker pronounced before\n",
    "\n",
    "\n",
    "    for start, end in child_intervals:\n",
    "\n",
    "        rep_int= (start, end)\n",
    "\n",
    "        start_rep = start * 1000\n",
    "        end_rep = end * 1000\n",
    "        rep_segment = audio[start_rep:end_rep]\n",
    "\n",
    "        rep_features= []\n",
    "\n",
    "        # Get features for repetitive utterance candidate:\n",
    "\n",
    "        for feature in features:\n",
    "            # use function 'get_feature_array()' to get features from 'Audiosegment' object\n",
    "            # Iterate through the 'features' list (= parameter of the function 'feature_df_echolalia()') \n",
    "            # to compute all the features\n",
    "            rep_feature= get_feature_array(feature, rep_segment, sr)\n",
    "\n",
    "            # Store all feature arrays in a list\n",
    "            rep_features.append(rep_feature)\n",
    "\n",
    "        # Concatenate different combinations of feature arrays\n",
    "        rep_combinations = [np.concatenate((f1, f2)) for f1, f2 in itertools.combinations(rep_features, 2)]\n",
    "        rep_all_concatenated= np.concatenate([feature for feature in rep_features]) \n",
    "\n",
    "\n",
    "        # Now store data of previous utterances:\n",
    "        for s, e in child_intervals:\n",
    "\n",
    "            source_int= (s,e)\n",
    "\n",
    "            start_source = s * 1000\n",
    "            end_source = e * 1000\n",
    "\n",
    "            # Store only data from utterances that start before the repetition candidate:\n",
    "            if start_source < start_rep: \n",
    "\n",
    "                source_segment = audio[start_source:end_source]\n",
    "                source_features= []\n",
    "\n",
    "                # Get features for source utterance:\n",
    "\n",
    "                for feature in features:\n",
    "\n",
    "                    source_feature= get_feature_array(feature, source_segment, sr)\n",
    "\n",
    "                    # Store all feature arrays in a list\n",
    "                    source_features.append(source_feature)\n",
    "\n",
    "                # Concatenate different combinations of feature arrays\n",
    "                source_combinations = [np.concatenate((f1, f2)) for f1, f2 in itertools.combinations(source_features, 2)]\n",
    "                source_all_concatenated= np.concatenate([feature for feature in source_features])\n",
    "\n",
    "\n",
    "                # Get distance metrics for the utterance pair:\n",
    "\n",
    "                distance_features= []\n",
    "\n",
    "                # Iterate over lists of feature arrays of both speakers and compute the normalized distance\n",
    "\n",
    "                for rep_feature, source_feature in zip(rep_features, source_features):\n",
    "                    distance_feature= dtw(rep_feature.T, source_feature.T, distance_only=True).normalizedDistance\n",
    "                    distance_features.append(distance_feature)\n",
    "\n",
    "                for rep_combination, source_combination in zip(rep_combinations, source_combinations):\n",
    "                    distance_feature= dtw(rep_combination.T, source_combination.T, distance_only=True).normalizedDistance\n",
    "                    distance_features.append(distance_feature)\n",
    "\n",
    "                # dtw computation on concatenation of all feature arrays:\n",
    "                dtw_combination= dtw(rep_all_concatenated.T, source_all_concatenated.T, distance_only=True).normalizedDistance\n",
    "\n",
    "\n",
    "\n",
    "                # ADD EVERYTHING TO THE DATAFRAME\n",
    "                df.loc[row_df] = [(source_int), (rep_int)] + distance_features + [dtw_combination]\n",
    "                # Move to a new row in the output dataframe and start a new iteration of the intervals of speaker 2\n",
    "                row_df+=1\n",
    "        \n",
    "        \n",
    "    # MAKE PREDICTIONS\n",
    "\n",
    "    # Load pretrained model and configurations\n",
    "    with open('Trained_best_classifier_self-repetition.pkl', 'rb') as f:\n",
    "        model_config = pickle.load(f)\n",
    "    model= model_config['model_obj']\n",
    "    best_features= model_config['best_features']\n",
    "    threshold= model_config['threshold']\n",
    "\n",
    "    # Define function for predictions\n",
    "    def predict(model, X, threshold):\n",
    "        probs = model.predict_proba(X) \n",
    "        return (probs[:, 1] > threshold).astype(int)\n",
    "\n",
    "    # Predict outcomes\n",
    "    X= df[best_features]\n",
    "    preds= pd.DataFrame(predict(model=model, X=X, threshold=threshold), columns= ['prediction_binary'])\n",
    "    df= pd.concat([df, preds], axis=1)\n",
    "    df['prediction'] = df['prediction_binary'].apply(lambda x: 'repetitive' if x == 1 else 'non-repetitive')\n",
    "    \n",
    "    \n",
    "    # MAKE TEXTGRID\n",
    "    tg = tgt.io.read_textgrid(textgrid_file, encoding='utf-16')\n",
    "    source_tier = tgt.IntervalTier(start_time=0, name='source')\n",
    "    rep_tier = tgt.IntervalTier(start_time=0, name='repetition')\n",
    "    \n",
    "    all_sources=[]\n",
    "    all_echoes=[]\n",
    "    \n",
    "    df_rep= df.loc[df['prediction']== 'repetitive']\n",
    "    \n",
    "    for index, row in df_rep.iterrows():\n",
    "        start_rep, end_rep = row['rep_int']\n",
    "        start_source, end_source = row['source_int']\n",
    "        \n",
    "        if (start_source, end_source) not in all_sources:\n",
    "            all_sources.append((start_source, end_source))\n",
    "            source_interval= tgt.Interval(start_time=float(start_source), end_time=float(end_source), text= 'source')\n",
    "            source_tier.add_interval(source_interval)\n",
    "        \n",
    "        if (start_rep, end_rep) not in all_echoes:\n",
    "            all_echoes.append((start_rep, end_rep))\n",
    "            rep_interval= tgt.Interval(start_time=float(start_rep), end_time=float(end_rep), text= 'repetition')\n",
    "            rep_tier.add_interval(rep_interval)\n",
    "            \n",
    "    # Output the TextGrid\n",
    "    tg.add_tier(source_tier)\n",
    "    tg.add_tier(rep_tier)\n",
    "    tgt.write_to_file(tg, output_path, format='short')\n",
    "            \n",
    "        \n",
    "    return df[['source_int', 'rep_int', 'prediction']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
